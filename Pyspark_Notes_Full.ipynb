{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rraushan378/Pyspark/blob/main/Pyspark_Notes_Full.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4dIQrhFlt1O",
        "outputId": "f010f856-a5d2-41f4-d32f-73d20ac95b3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=8ce0911ba3d573d7234b8f2c11fe0eedb19ec5921b3773dc2665b5d5eb5d3309\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "#!pip install pyspark py4j"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8H33Ktanbb6"
      },
      "source": [
        "**Apache Spark**:\n",
        "\n",
        "Apache Spark is an open-source, In- Memory and distributed computing system designed for big data processing and analytics.It is written in scala. Spark is known for its speed and efficiency.\n",
        "\n",
        "**Spark Architecture**\n",
        "\n",
        "The Spark driver:\n",
        "\n",
        "The driver is the program or process responsible for coordinating the execution of the Spark application. It runs the main function and creates the SparkContext, which connects to the cluster manager.\n",
        "\n",
        "The Spark executors:\n",
        "\n",
        "Executors are worker processes responsible for executing tasks in Spark applications. They are launched on worker nodes and communicate with the driver program and cluster manager. Executors run tasks concurrently and store data in memory or disk for caching and intermediate storage.\n",
        "\n",
        "The cluster manager:\n",
        "\n",
        "The cluster manager is responsible for allocating resources and managing the cluster on which the Spark application runs. Spark supports various cluster managers like Apache Mesos, Hadoop YARN, and standalone cluster manager.\n",
        "\n",
        "What is SparkSession?\n",
        "\n",
        "SparkSession was introduced in version Spark 2.0, it is an entry point to underlying Spark functionality in order to programmatically create Spark RDD, DataFrame, and DataSet. SparkSession’s object spark is the default variable available in spark-shell and it can be created programmatically using SparkSession builder pattern.\n",
        "\n",
        "\n",
        "**Task**:\n",
        "\n",
        "A task is the smallest unit of work in Spark, representing a unit of computation that can be performed on a single partition of data. The driver program divides the Spark job into tasks and assigns them to the executor nodes for execution.\n",
        "\n",
        "**Working of Spark Architecture**:\n",
        "\n",
        "When the Driver Program in the Apache Spark architecture executes, it calls the real program of an application and creates a SparkContext. SparkContext contains all of the basic functions. The Spark Driver includes several other components, including a DAG Scheduler, Task Scheduler, Backend Scheduler, and Block Manager, all of which are responsible for translating user-written code into jobs that are actually executed on the cluster.\n",
        "\n",
        "The Cluster Manager manages the execution of various jobs in the cluster. Spark Driver works in conjunction with the Cluster Manager to control the execution of various other jobs. The cluster Manager does the task of allocating resources for the job. Once the job has been broken down into smaller Tasks, which are then distributed to worker nodes, SparkDriver will control the execution.\n",
        "Many worker nodes can be used to process an RDD(Resilient Distributed Dataset) created in SparkContext, and the results can also be cached.\n",
        "\n",
        "The SparkContext receives task information from the Cluster Manager and enqueues it on worker nodes. The executor is in charge of carrying out these duties. The lifespan of executors is the same as that of the Spark Application. We can increase the number of workers if we want to improve the performance of the system.\n",
        "\n",
        "**Cluster Manager Types**\n",
        "\n",
        "The system currently supports several cluster managers:\n",
        "\n",
        "**Standalone** — spark has their own cluster data are stored in local file system.\n",
        "**Apache Mesos** — a general cluster manager that can also run Hadoop MapReduce and service applications.\n",
        "**Hadoop YARN** — Spark on Hadoop use HDFS and Yarn features for processing\n",
        "**Kubernetes** — an open-source system for automating deployment, scaling, and management of containerized applications.\n",
        "\n",
        "\n",
        "**Execution Modes**\n",
        "\n",
        "**Cluster mode**:\n",
        "\n",
        "Cluster mode is probably the most common way of running Spark Applications. In cluster mode, a user submits a pre-compiled JAR, Python script, or R script to a cluster manager. The cluster manager then launches the driver process on a worker node inside the cluster, in addition to the executor processes. This means that the cluster manager is responsible for maintaining all Spark Application related processes.\n",
        "\n",
        "**Client mode**:\n",
        "\n",
        "Client mode is nearly the same as cluster mode except that the Spark driver remains on the client machine that submitted the application. This means if client sumit the spark application and got log out from machine the execution will stop and will not be completed.\n",
        "\n",
        "**Local mode**:\n",
        "\n",
        "Local mode is a significant departure from the previous two modes: it runs the entire Spark Application on a single machine. It achieves parallelism through threads on that single machine. This is a common way to learn Spark, test your applications, or experiment iteratively with local development."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9wDKj8vl-qG"
      },
      "source": [
        "sparkContext:\n",
        "\n",
        "*pyspark.SparkContext is an entry point to the PySpark functionality that is used to communicate with the cluster and to create an RDD, accumulator, and broadcast variable*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5baaST1TVLu",
        "outputId": "fe10c166-980b-43c4-d5aa-c15e943e731a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Partitions: 5\n",
            "Action: First element: 1\n",
            "[1, 2, 3, 4, 5]\n"
          ]
        }
      ],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').master(\"local[5]\").getOrCreate()\n",
        "sparkContext=spark.sparkContext\n",
        "rdd=sparkContext.parallelize([1,2,3,4,5])\n",
        "rddCollect = rdd.collect()\n",
        "print(\"Number of Partitions: \"+str(rdd.getNumPartitions()))\n",
        "print(\"Action: First element: \"+str(rdd.first()))\n",
        "print(rddCollect)\n",
        "#sparkContext.stop()\n",
        "'''you can create only one SparkContext per JVM, in order to create another first you need to stop the existing one using stop() method'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfoR8z8AnkOr"
      },
      "source": [
        "RDD Introduction\n",
        "\n",
        "RDD (Resilient Distributed Dataset) is a core building block of PySpark. It is a fault-tolerant, immutable, distributed collection of objects. Immutable means that once you create an RDD, you cannot change it. The data within RDDs is segmented into logical partitions, allowing for distributed computation across multiple nodes within the cluster.\n",
        "\n",
        "* it is a collection of rows without schema\n",
        "* it is immutable\n",
        "* it is lazy execuation at row level and eager execution at schema level\n",
        "\n",
        "\n",
        "**Spark DataFrame**\n",
        "\n",
        "In Spark Scala, a DataFrame is a distributed collection of data organized into named columns similar to an SQL table.\n",
        "\n",
        "* It is similar to a table in a relational database or a spreadsheet in that it has a schema, which defines the types and names of its columns, and each row represents a single record or observation.\n",
        "* DataFrames in Spark Scala can be created from a variety of sources, such as RDDs, structured data files (e.g., CSV, JSON, Parquet), Hive tables, or external databases\n",
        "* Once created, DataFrames support a wide range of operations and transformations, such as filtering, aggregating, joining, and grouping data.\n",
        "* One of the key benefits of using DataFrames in Spark Scala is their ability to leverage Spark’s distributed computing capabilities to process large amounts of data quickly and efficiently.\n",
        "\n",
        "\n",
        "RDD Creation\n",
        "\n",
        "Using sparkContext.parallelize()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3ZHksE-Ttnc",
        "outputId": "c8c402e6-d221-48a7-e8d2-3c3f98932ff2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n"
          ]
        }
      ],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.master(\"local[1]\").getOrCreate()\n",
        "sc=spark.sparkContext\n",
        "data=[1,2,3,4,5,6,7,8,9,10,11,12]\n",
        "rdd=sc.parallelize(data)\n",
        "print(rdd.collect())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnH5LoxQtoC-"
      },
      "source": [
        "Using File Location:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bj0bIvDTpyyb",
        "outputId": "b2c6af1d-96f0-4054-dccc-9b00cfed622e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10000\n"
          ]
        }
      ],
      "source": [
        "rdd = spark.sparkContext.textFile(\"sample_data/mnist_test.csv\")\n",
        "print(rdd.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxrluZ8UPnm4",
        "outputId": "56c0f625-9033-4771-84fd-b0a445f2c71e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----+-----+---------------+\n",
            "|Series|   X|    Y|_corrupt_record|\n",
            "+------+----+-----+---------------+\n",
            "|  NULL|NULL| NULL|              [|\n",
            "|     I|10.0| 8.04|           NULL|\n",
            "|     I| 8.0| 6.95|           NULL|\n",
            "|     I|13.0| 7.58|           NULL|\n",
            "|     I| 9.0| 8.81|           NULL|\n",
            "|     I|11.0| 8.33|           NULL|\n",
            "|     I|14.0| 9.96|           NULL|\n",
            "|     I| 6.0| 7.24|           NULL|\n",
            "|     I| 4.0| 4.26|           NULL|\n",
            "|     I|12.0|10.84|           NULL|\n",
            "|     I| 7.0| 4.81|           NULL|\n",
            "|     I| 5.0| 5.68|           NULL|\n",
            "|    II|10.0| 9.14|           NULL|\n",
            "|    II| 8.0| 8.14|           NULL|\n",
            "|    II|13.0| 8.74|           NULL|\n",
            "|    II| 9.0| 8.77|           NULL|\n",
            "|    II|11.0| 9.26|           NULL|\n",
            "|    II|14.0|  8.1|           NULL|\n",
            "|    II| 6.0| 6.13|           NULL|\n",
            "|    II| 4.0|  3.1|           NULL|\n",
            "+------+----+-----+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "rdd1=spark.read.csv(path=\"sample_data/mnist_test.csv\",header=True,inferSchema=True)\n",
        "rdd1.show(1)\n",
        "rdd2=spark.read.json(path=\"sample_data/anscombe.json\")\n",
        "rdd2.show()\n",
        "# Example of using option()\n",
        "#default is patquet\n",
        "rdd2.write.format(\"csv\")  \\\n",
        "        .option(\"header\", \"true\")  \\\n",
        "        .option(\"delimiter\", \"|\")  \\\n",
        "        .save(\"sample_data/output\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fvPIxRFup9s"
      },
      "source": [
        " PySpark RDD Repartition() vs Coalesce():\n",
        "\n",
        "In PySpark, the choice between repartition() and coalesce() functions carries importance in optimizing performance and resource utilization. These methods play pivotal roles in reshuffling data across partitions within a DataFrame, yet they differ in their mechanisms and implications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKWpvTtcu36a",
        "outputId": "18d7e58a-d3e4-4dbb-cb19-d755c08a0ba5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "From local[5] : 5\n",
            "parallelize : 6\n"
          ]
        }
      ],
      "source": [
        "# Create spark session with local[5]\n",
        "rdd = spark.sparkContext.parallelize(range(0,20))\n",
        "print(\"From local[5] : \"+str(rdd.getNumPartitions()))\n",
        "\n",
        "# Use parallelize with 6 partitions\n",
        "rdd1 = spark.sparkContext.parallelize(range(0,25), 6)\n",
        "print(\"parallelize : \"+str(rdd1.getNumPartitions()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is46WJ7OvIY7"
      },
      "source": [
        "RDD repartition():\n",
        "\n",
        "repartition() is a transformation method available on RDDs (Resilient Distributed Datasets) that redistributes data across a specified number of partitions. When you call repartition(n), where n is the desired number of partitions, Spark reshuffles the data in the RDD into exactly n partitions.\n",
        "\n",
        "\n",
        "* it increase or decrese the no of partition with redistribution of all data across the specified no of partition.\n",
        "* Full shuffling is done in repartition()\n",
        "* Can be expensive for large dataset.\n",
        "* Distribute data across partition evenly(means same size of each partition)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNQKZfx4vKXc",
        "outputId": "fc232f98-e1e3-4fb6-d6fd-bf20f4b64580"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Repartition size : 4\n"
          ]
        }
      ],
      "source": [
        "rdd2 = rdd1.repartition(4)\n",
        "print(\"Repartition size : \"+str(rdd2.getNumPartitions()))\n",
        "rdd2.saveAsTextFile(\"re-partition.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUG-pznsvn0n"
      },
      "source": [
        "RDD coalesce():\n",
        "\n",
        "\n",
        "In PySpark, coalesce() is a transformation method available on RDDs (Resilient Distributed Datasets) that reduces the number of partitions without shuffling data across the cluster. When you call coalesce(n), where n is the desired number of partitions, Spark merges existing partitions to create n partitions.\n",
        "\n",
        "\n",
        "* It decrese the no of partition without shuffling of all data and by merging existing partition.\n",
        "* Shuffling is not done in Coalesce().\n",
        "* Less expensive then repartition().\n",
        "* Distribute data acress partition of imbalance size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMRGUPzzvpPh"
      },
      "outputs": [],
      "source": [
        "rdd3 = rdd1.coalesce(4)\n",
        "print(\"Repartition size : \"+str(rdd3.getNumPartitions()))\n",
        "#rdd3.saveAsTextFile(\"/tmp/coalesce\")\n",
        "\n",
        "'''Partition 1 : 0 1 2\n",
        "Partition 2 : 3 4 5 6 7 8 9\n",
        "Partition 4 : 10 11 12\n",
        "Partition 5 : 13 14 15 16 17 18 19'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEm2Nhr0wUSD"
      },
      "source": [
        "Broadcast Variables:\n",
        "\n",
        "Broadcast variables are read-only shared variables that are cached and available on all nodes in a cluster in-order to access or use by the tasks. Instead of sending this data along with every task, PySpark distributes broadcast variables to the workers using efficient broadcast algorithms to reduce communication costs.\n",
        "\n",
        "\n",
        "How does PySpark Broadcast work?\n",
        "\n",
        "When you run a PySpark RDD, DataFrame applications that have the Broadcast variables defined and used, PySpark does the following.\n",
        "\n",
        "* PySpark breaks the job into stages that have distributed shuffling and actions are executed with in the stage.\n",
        "\n",
        "* Later Stages are also broken into tasks\n",
        "\n",
        "* Spark broadcasts the common data (reusable) needed by tasks within each stage.\n",
        "\n",
        "* The broadcasted data is cache in serialized format and deserialized before executing each task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIlB6cyuwYDw",
        "outputId": "f3e10e2f-8ef5-48d6-9131-b838bee55a6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('James', 'Smith', 'USA', 'California'), ('Michael', 'Rose', 'USA', 'New York'), ('Robert', 'Williams', 'USA', 'California'), ('Maria', 'Jones', 'USA', 'Florida')]\n"
          ]
        }
      ],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
        "broadcastStates = spark.sparkContext.broadcast(states)\n",
        "\n",
        "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
        "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
        "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
        "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
        "  ]\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "def state_convert(code):\n",
        "    return broadcastStates.value[code]\n",
        "\n",
        "result = rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).collect()\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ottSU_JyweCP"
      },
      "source": [
        "Accumulator:\n",
        "\n",
        "The PySpark Accumulator is a shared variable that is used with RDD and DataFrame to perform sum and counter operations similar to Map-reduce counters. These variables are shared by all executors to update and add information through aggregation or computative operations.\n",
        "\n",
        "\n",
        "What is PySpark Accumulator?\n",
        "\n",
        "Accumulators are write-only and initialize once variables where only tasks that are running on workers are allowed to update and updates from the workers get propagated automatically to the driver program. But, only the driver program is allowed to access the Accumulator variable using the value property"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sVQCw6vxDG4",
        "outputId": "dbf3cd0a-1768-45d6-a53f-a837a5dccbf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName(\"accumulator\").getOrCreate()\n",
        "\n",
        "accum=spark.sparkContext.accumulator(0)\n",
        "rdd=spark.sparkContext.parallelize([1,2,3,4,5])\n",
        "rdd.foreach(lambda x:accum.add(x))\n",
        "print(accum.value) #Accessed by driver"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35ipnPbWxWLN"
      },
      "source": [
        "Create Empty RDD in PySpark:\n",
        "\n",
        "Create an empty RDD by using emptyRDD() of SparkContext for example spark.sparkContext.emptyRDD()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DI8eptL3xOHG",
        "outputId": "abcae8b8-fed5-4813-ac22-56f280592672"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EmptyRDD[32] at emptyRDD at NativeMethodAccessorImpl.java:0\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "#Creates Empty RDD\n",
        "emptyRDD = spark.sparkContext.emptyRDD()\n",
        "print(emptyRDD)\n",
        "\n",
        "#Diplays\n",
        "#EmptyRDD[188] at emptyRDD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsEYyWJVxkKB"
      },
      "source": [
        "Create Empty DataFrame with Schema (StructType):\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgDV5ixwxoL7"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StructType,StructField, StringType\n",
        "schema = StructType([\n",
        "  StructField('firstname', StringType(), True),\n",
        "  StructField('middlename', StringType(), True),\n",
        "  StructField('lastname', StringType(), True)\n",
        "  ])\n",
        "\n",
        "#Create empty DataFrame from empty RDD\n",
        "df = spark.createDataFrame(emptyRDD,schema)\n",
        "df.printSchema()\n",
        "#convert RDD to DF\n",
        "df1 = emptyRDD.toDF(schema)\n",
        "df1.printSchema()\n",
        "\n",
        "#Create empty DataFrame directly.\n",
        "df2 = spark.createDataFrame([], schema)\n",
        "df2.printSchema()\n",
        "\n",
        "\n",
        "#Create empty DatFrame with no schema (no columns)\n",
        "df3 = spark.createDataFrame([], StructType([]))\n",
        "df3.printSchema()\n",
        "\n",
        "#print below empty schema\n",
        "#root"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TaL6UltyFYQ"
      },
      "source": [
        "Convert RDD to DF:\n",
        "\n",
        "1.Using rdd.toDF() function\n",
        "PySpark provides toDF() function in RDD which can be used to convert RDD into Dataframe\n",
        "\n",
        "2.Using PySpark createDataFrame() function\n",
        "\n",
        "SparkSession class provides createDataFrame() method to create DataFrame and it takes rdd object as an argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Re6Kco80yIS2",
        "outputId": "ccbe3dd4-7fa1-45b8-b8e8-234debd9252d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- _1: string (nullable = true)\n",
            " |-- _2: long (nullable = true)\n",
            "\n",
            "+---------+---+\n",
            "|_1       |_2 |\n",
            "+---------+---+\n",
            "|Finance  |10 |\n",
            "|Marketing|20 |\n",
            "|Sales    |30 |\n",
            "|IT       |40 |\n",
            "+---------+---+\n",
            "\n",
            "root\n",
            " |-- dept_name: string (nullable = true)\n",
            " |-- dept_id: long (nullable = true)\n",
            "\n",
            "+---------+-------+\n",
            "|dept_name|dept_id|\n",
            "+---------+-------+\n",
            "|Finance  |10     |\n",
            "|Marketing|20     |\n",
            "|Sales    |30     |\n",
            "|IT       |40     |\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
        "rdd = spark.sparkContext.parallelize(dept)\n",
        "#using toDF function\n",
        "df = rdd.toDF()\n",
        "df.printSchema()\n",
        "df.show(truncate=False)\n",
        "# with selected columns\n",
        "deptColumns = [\"dept_name\",\"dept_id\"]\n",
        "df2 = rdd.toDF(deptColumns)\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False)\n",
        "#using createDataFrame function\n",
        "'''deptSchema = StructType([\n",
        "    StructField('dept_name', StringType(), True),\n",
        "    StructField('dept_id', StringType(), True)\n",
        "])'''\n",
        "df3 = spark.createDataFrame(rdd,[\"dept_name\",\"dept_id\"]) #or (rdd,schema=deptSchema)\n",
        "df3.printSchema()\n",
        "df3.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJkP-fHoy7Jh"
      },
      "source": [
        "**Convert PySpark Dataframe to Pandas DataFrame**\n",
        "\n",
        "PySpark DataFrame provides a method toPandas() to convert it to Python Pandas DataFrame.\n",
        "\n",
        "toPandas() results in the collection of all records in the PySpark DataFrame to the driver program and should be done only on a small subset of the data. running on larger dataset’s results in memory error and crashes the application. To deal with a larger dataset, you can also try increasing memory on the driver.\n",
        "\n",
        "StructType – Defines the structure of the DataFrame\n",
        "\n",
        "StructField – Defines the metadata of the DataFrame column\n",
        "\n",
        "It represents a field in the schema, containing metadata such as the name, data type, and nullable status of the field. Each StructField object defines a single column in the DataFrame, specifying its name and the type of data it holds.\n",
        "You can Check Below Example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGyIPOZlz6ID"
      },
      "outputs": [],
      "source": [
        "# Nested structure elements\n",
        "import pyspark\n",
        "from pyspark.sql.types import StructType, StructField, StringType,IntegerType\n",
        "dataStruct = [((\"James\",\"\",\"Smith\"),\"36636\",\"M\",\"3000\"), \\\n",
        "      ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",\"4000\"), \\\n",
        "      ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",\"4000\"), \\\n",
        "      ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",\"4000\"), \\\n",
        "      ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",\"-1\") \\\n",
        "]\n",
        "\n",
        "schemaStruct = StructType([\n",
        "        StructField('name', StructType([\n",
        "             StructField('firstname', StringType(), True),\\\n",
        "             StructField('middlename', StringType(), True),\\\n",
        "             StructField('lastname', StringType(), True)\\\n",
        "             ])),\\\n",
        "          StructField('dob', StringType(), True),\\\n",
        "         StructField('gender', StringType(), True),\\\n",
        "         StructField('salary', StringType(), True)\\\n",
        "         ])\n",
        "df = spark.createDataFrame(data=dataStruct, schema = schemaStruct)\n",
        "df.printSchema()\n",
        "#using toPandas()\n",
        "pandasDF2 = df.toPandas()\n",
        "print(pandasDF2)\n",
        "\n",
        "# Default - displays 20 rows and\n",
        "# 20 charactes from column value\n",
        "df.show()\n",
        "\n",
        "#Display full column contents\n",
        "df.show(truncate=False)\n",
        "\n",
        "# Display 2 rows and full column contents\n",
        "df.show(2,truncate=False)\n",
        "\n",
        "# Display 2 rows & column values 25 characters\n",
        "df.show(2,truncate=25)\n",
        "\n",
        "# Display DataFrame rows & columns vertically\n",
        "df.show(n=3,truncate=25,vertical=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yF9gHXqM2mkT"
      },
      "outputs": [],
      "source": [
        "#Create DataFrame with struct using Row class\n",
        "from pyspark.sql import Row\n",
        "data=[Row(name=\"James\",prop=Row(hair=\"black\",eye=\"blue\")),\n",
        "      Row(name=\"Ann\",prop=Row(hair=\"grey\",eye=\"black\"))]\n",
        "df=spark.createDataFrame(data)\n",
        "df.printSchema()\n",
        "#root\n",
        "# |-- name: string (nullable = true)\n",
        "# |-- prop: struct (nullable = true)\n",
        "# |    |-- hair: string (nullable = true)\n",
        "# |    |-- eye: string (nullable = true)\n",
        "\n",
        "#Access struct column\n",
        "df.select(df.prop.hair).show()\n",
        "df.select(df[\"prop.hair\"]).show()\n",
        "df.select(col(\"prop.hair\")).show()\n",
        "\n",
        "#Access all columns from struct\n",
        "df.select(col(\"prop.*\")).show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8x1dt7R23Jb"
      },
      "source": [
        "Column Functions:\n",
        "\n",
        "Arithmetic(),\n",
        "alias(),\n",
        "isin(),\n",
        "asc(),\n",
        "desc(),\n",
        "contains(),\n",
        "between(),\n",
        "cast(),\n",
        "like(),\n",
        "substring(),\n",
        "when() & otherwise()\n",
        "\n",
        "**select**() function:\n",
        "\n",
        " It is used to select single, multiple, column by index, all columns from the list and the nested columns from a DataFrame, PySpark select() is a transformation function hence it returns a new DataFrame with the selected columns.\n",
        "\n",
        " Collect():\n",
        "\n",
        " collect() function of the RDD/DataFrame is an action operation that returns all elements of the DataFrame\n",
        "\n",
        "\n",
        "collect () vs select ()\n",
        "\n",
        "select() is a transformation that returns a new DataFrame and holds the columns that are selected whereas collect() is an action that returns the entire data set in an Array to the driver."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmRx0PcS26La"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import expr\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "data=[(100,2,1),(200,3,4),(300,4,4)]\n",
        "df=spark.createDataFrame(data).toDF(\"col1\",\"col2\",\"col3\")\n",
        "\n",
        "#Arthmetic operations\n",
        "df.select(df.col1 + df.col2).show()\n",
        "'''df.select(df.col1 - df.col2).show()\n",
        "df.select(df.col1 * df.col2).show()\n",
        "df.select(df.col1 / df.col2).show()\n",
        "df.select(df.col1 % df.col2).show()\n",
        "df.select(df.col2 > df.col3).show()\n",
        "df.select(df.col2 < df.col3).show()\n",
        "df.select(df.col2 == df.col3).show()'''\n",
        "\n",
        "data=[(\"James\",\"Bond\",\"100\",None),\n",
        "      (\"Ann\",\"Varsa\",\"200\",'F'),\n",
        "      (\"Tom Cruise\",\"XXX\",\"400\",''),\n",
        "      (\"Tom Brand\",None,\"400\",'M')]\n",
        "columns=[\"fname\",\"lname\",\"id\",\"gender\"]\n",
        "df=spark.createDataFrame(data,columns)\n",
        "#show with alias() function\n",
        "df.select(df.fname.alias(\"first_name\"), \\\n",
        "          df.lname.alias(\"last_name\")\n",
        "   ).show(1)\n",
        "\n",
        "df.select(expr(\" fname ||','|| lname\").alias(\"fullName\")).show(1) #import expr\n",
        "#asc, desc to sort ascending and descending order repsectively.\n",
        "df.sort(df.fname.asc()).show()\n",
        "df.sort(df.fname.desc()).show()\n",
        "df.sort(\"fname\", \"lname\", ascending=[True, False]) \\\n",
        "  .show()\n",
        "#print('Ordery function')\n",
        "df.orderBy(col(\"fname\").asc(),col(\"lname\").asc()).show(truncate=False)\n",
        "\n",
        "print('Sort using spark ')\n",
        "\n",
        "df.createOrReplaceTempView(\"EMP\")\n",
        "spark.sql(\"select fname,lname,id,gender from EMP ORDER BY fname asc\").show(truncate=False)\n",
        "\n",
        "\n",
        "#cast\n",
        "df.select(df.fname,df.id.cast(\"int\")).printSchema()\n",
        "#between\n",
        "df.filter(df.id.between(100,300)).show()\n",
        "#contains\n",
        "df.filter(df.fname.contains(\"Cruise\")).show()\n",
        "#startswith, endswith()\n",
        "df.filter(df.fname.startswith(\"T\")).show()\n",
        "df.filter(df.fname.endswith(\"Cruise\")).show()\n",
        "#isNull & isNotNull\n",
        "df.filter(df.lname.isNull()).show()\n",
        "df.filter(df.lname.isNotNull()).show()\n",
        "#like , rlike\n",
        "df.select(df.fname,df.lname,df.id) \\\n",
        "  .filter(df.fname.like(\"%om\"))\n",
        "#Substring\n",
        "df.select(df.fname.substr(1,2).alias(\"substr\")).show()\n",
        "#when & otherwise\n",
        "from pyspark.sql.functions import when\n",
        "df.select(df.fname,df.lname,when(df.gender==\"M\",\"Male\") \\\n",
        "              .when(df.gender==\"F\",\"Female\") \\\n",
        "              .when(df.gender==None ,\"\") \\\n",
        "              .otherwise(df.gender).alias(\"new_gender\") \\\n",
        "    ).show()\n",
        "#isin\n",
        "li=[\"100\",\"200\"]\n",
        "df.select(df.fname,df.lname,df.id) \\\n",
        "  .filter(df.id.isin(li)) \\\n",
        "  .show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF9JB_TN7To0"
      },
      "source": [
        "withColumn():\n",
        "\n",
        " It is a transformation function of DataFrame which is used to change the value, convert the datatype of an existing column, create a new column, and many more.\n",
        "\n",
        "\n",
        " What is the difference between where and filter in PySpark?\n",
        "\n",
        "In PySpark, both filter() and where() functions are used to select out data based on certain conditions. They are used interchangeably, and both of them essentially perform the same operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQe7QxUT7dNO",
        "outputId": "9813e32c-2720-4aac-8d25-2f45e3869edb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-----------+\n",
            "|gender|sum(salary)|\n",
            "+------+-----------+\n",
            "|M     |11000      |\n",
            "|F     |3999       |\n",
            "+------+-----------+\n",
            "\n",
            "+------+----------+------------------+-------+-------+\n",
            "|gender|sum_salary|avg_salary        |sum_sal|max_sal|\n",
            "+------+----------+------------------+-------+-------+\n",
            "|M     |11000     |3666.6666666666665|11000  |4000   |\n",
            "|F     |3999      |1999.5            |3999   |4000   |\n",
            "+------+----------+------------------+-------+-------+\n",
            "\n",
            "+------+----------+------------------+-------+-------+\n",
            "|gender|sum_salary|        avg_salary|sum_sal|max_sal|\n",
            "+------+----------+------------------+-------+-------+\n",
            "|     M|     11000|3666.6666666666665|  11000|   4000|\n",
            "|     F|      3999|            1999.5|   3999|   4000|\n",
            "+------+----------+------------------+-------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import expr,col\n",
        "from pyspark.sql.functions import sum,avg,max\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "data = [('James','','Smith','1991-04-01','M',3000),\n",
        "  ('Michael','Rose','','2000-05-19','M',4000),\n",
        "  ('Robert','','Williams','1978-09-05','M',4000),\n",
        "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
        "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
        "]\n",
        "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
        "df = spark.createDataFrame(data=data, schema = columns)\n",
        "df.withColumn(\"salary\",col(\"salary\").cast(\"Integer\")).show()\n",
        "#Update The Value of an Existing Column\n",
        "df.withColumn(\"salary\",col(\"salary\")*100).show()\n",
        "#Create a Column from an Existing\n",
        "df.withColumn(\"CopiedColumn\",col(\"salary\")* -1).show()\n",
        "#Rename Column\n",
        "df.withColumnRenamed(\"gender\",\"sex\") \\\n",
        "  .show(truncate=False)\n",
        "#drop column\n",
        "df.drop(\"salary\") \\\n",
        "  .show()\n",
        "# Remove duplicates on selected columns using dropDuplicates()\n",
        "dropDisDF = df.dropDuplicates([\"gender\",\"salary\"])\n",
        "print(\"Distinct count of gender & salary : \"+str(dropDisDF.count()))\n",
        "dropDisDF.show(truncate=False)\n",
        "\n",
        "# Sort using spark SQL\n",
        "\n",
        "df.createOrReplaceTempView(\"EMP\")\n",
        "spark.sql(\"select employee_name,department,state,salary,age,bonus from EMP ORDER BY department asc\").show(truncate=False)\n",
        "\n",
        "\n",
        "#filter\n",
        "# Using equal condition\n",
        "df.filter(df.salary == 3000).show(truncate=False)\n",
        "# Using SQL Expression\n",
        "df.filter(\"gender == 'M'\").show()\n",
        "# Filter multiple conditions\n",
        "df.filter( (df.lastname  == \"Smith\") & (df.gender  == \"M\") ) \\\n",
        "    .show(truncate=False)\n",
        "df.filter(df.firstname.like(\"%ber%\")).show()\n",
        "\n",
        "# Using groupBy().sum()\n",
        "df.groupBy(\"gender\").sum(\"salary\").show(truncate=False)\n",
        "# Using filter on aggregate data\n",
        "df.groupBy(\"gender\") \\\n",
        "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
        "      avg(\"salary\").alias(\"avg_salary\"), \\\n",
        "      sum(\"salary\").alias(\"sum_sal\"), \\\n",
        "      max(\"salary\").alias(\"max_sal\")) \\\n",
        "    .where(col(\"sum_salary\") >= 500) \\\n",
        "    .show(truncate=False)\n",
        "\n",
        "# Register DataFrame as a temporary view\n",
        "df.createOrReplaceTempView(\"employees\")\n",
        "\n",
        "# Using SQL Query\n",
        "sql_string = \"\"\"SELECT gender,\n",
        "       SUM(salary) AS sum_salary,\n",
        "       AVG(salary) AS avg_salary,\n",
        "       SUM(salary) AS sum_sal,\n",
        "       MAX(salary) AS max_sal\n",
        "FROM employees\n",
        "GROUP BY gender\n",
        "HAVING SUM(salary) >= 1000\"\"\"\n",
        "\n",
        "# Execute SQL query against the temporary view\n",
        "df2 = spark.sql(sql_string)\n",
        "df2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLfNvet2HIL1"
      },
      "source": [
        "JOIN():\n",
        "\n",
        "How Join works?\n",
        "\n",
        "PySpark’s join operation combines data from two or more Datasets based on a common column or key. It is a fundamental operation in PySpark and is similar to SQL joins.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Bl14tB5x_GmM"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
        "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
        "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
        "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
        "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
        "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
        "  ]\n",
        "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
        "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
        "\n",
        "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
        "empDF.printSchema()\n",
        "empDF.show(truncate=False)\n",
        "\n",
        "\n",
        "dept = [(\"Finance\",10), \\\n",
        "    (\"Marketing\",20), \\\n",
        "    (\"Sales\",30), \\\n",
        "    (\"IT\",40) \\\n",
        "  ]\n",
        "deptColumns = [\"dept_name\",\"dept_id\"]\n",
        "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
        "deptDF.printSchema()\n",
        "deptDF.show(truncate=False)\n",
        "\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\") \\\n",
        "     .show(truncate=False)\n",
        "\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"left\") \\\n",
        "    .show(truncate=False)\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"right\") \\\n",
        "   .show(truncate=False)\n",
        "\n",
        "empDF.alias(\"emp1\").join(empDF.alias(\"emp2\"), \\\n",
        "    col(\"emp1.superior_emp_id\") == col(\"emp2.emp_id\"),\"inner\") \\\n",
        "    .select(col(\"emp1.emp_id\"),col(\"emp1.name\"), \\\n",
        "      col(\"emp2.emp_id\").alias(\"superior_emp_id\"), \\\n",
        "      col(\"emp2.name\").alias(\"superior_emp_name\")) \\\n",
        "   .show(truncate=False)\n",
        "\n",
        "empDF.createOrReplaceTempView(\"EMP\")\n",
        "deptDF.createOrReplaceTempView(\"DEPT\")\n",
        "\n",
        "joinDF = spark.sql(\"select * from EMP e, DEPT d where e.emp_dept_id == d.dept_id\") \\\n",
        "  .show(truncate=False)\n",
        "\n",
        "joinDF2 = spark.sql(\"select * from EMP e Left JOIN DEPT d ON e.emp_dept_id == d.dept_id\") \\\n",
        "  .show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyA4C3B4JqVz"
      },
      "source": [
        "Map() & FlatMap()\n",
        "\n",
        "The map()in PySpark is a transformation function that is used to apply a function/lambda to each element of an RDD (Resilient Distributed Dataset) and return a new RDD consisting of the result.\n",
        "* It is based on One-to-One\n",
        "Exemple:\n",
        "         rdd=sc.parallelize([5,6,7])\n",
        "         rdd.map(lambda x: [x,x,x]).show()\n",
        "         result=[[5,5,5],[6,6,6],[7,7,7]]\n",
        "\n",
        "\n",
        "\n",
        "PySpark flatMap(): is a transformation operation that flattens the RDD/DataFrame (array/map DataFrame columns) after applying the function on every element and returns a new PySpark RDD/DataFrame.\n",
        "* it is based on one-to-Many\n",
        "\n",
        "Exemple:\n",
        "\n",
        "         rdd=sc.parallelize([5,6,7])\n",
        "         rdd.map(lambda x: [x,x,x]).show()\n",
        "         result=[5,5,5,6,6,6,7,7,7]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndCfpNj8JssW",
        "outputId": "3c5ce051-08ed-42e9-9841-a05b03f65e1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Alice’s', 1),\n",
              " ('Gutenberg’s', 3),\n",
              " ('Adventures', 2),\n",
              " ('in', 2),\n",
              " ('Wonderland', 2),\n",
              " ('Project', 3)]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Imports\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[1]\") \\\n",
        "    .appName(\"SparkByExamples.com\").getOrCreate()\n",
        "\n",
        "data = [\"Project\",\"Gutenberg’s\",\"Alice’s\",\"Adventures\",\n",
        "\"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\",\"Adventures\",\n",
        "\"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\"]\n",
        "\n",
        "rdd=spark.sparkContext.parallelize(data)\n",
        "# map() with rdd\n",
        "rdd2=rdd.map(lambda x: (x,1))\n",
        "#for element in rdd2.collect():\n",
        " #   print(element)\n",
        "rdd3=rdd2.reduceByKey(lambda x,y:x+y)\n",
        "rdd3.collect()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9tr5avwK3xy",
        "outputId": "3e28c98e-5ed7-45bb-84f4-fed432e944af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project\n",
            "Gutenberg’s\n",
            "Alice’s\n",
            "Adventures\n",
            "in\n",
            "Wonderland\n",
            "Project\n",
            "Gutenberg’s\n",
            "Adventures\n",
            "in\n",
            "Wonderland\n",
            "Project\n",
            "Gutenberg’s\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[1]\") \\\n",
        "    .appName(\"SparkByExamples.com\").getOrCreate()\n",
        "\n",
        "data = [\"Project Gutenberg’s\",\n",
        "        \"Alice’s Adventures in Wonderland\",\n",
        "        \"Project Gutenberg’s\",\n",
        "        \"Adventures in Wonderland\",\n",
        "        \"Project Gutenberg’s\"]\n",
        "rdd=spark.sparkContext.parallelize(data)\n",
        "#for element in rdd.collect():\n",
        "  #  print(element)\n",
        "\n",
        "rdd2=rdd.flatMap(lambda x: x.split(\" \"))\n",
        "for element in rdd2.collect():\n",
        "    print(element)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axY2i-NHL6Ic"
      },
      "source": [
        "EXPLODE Function():"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDRi8CgbL4ov"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('pyspark-by-examples').getOrCreate()\n",
        "\n",
        "arrayData = [\n",
        "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
        "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
        "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
        "        ('Washington',None,None),\n",
        "        ('Jefferson',['1','2'],{})]\n",
        "df = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])\n",
        "\n",
        "from pyspark.sql.functions import explode\n",
        "df2 = df.select(df.name,explode(df.knownLanguages))\n",
        "df2.printSchema()\n",
        "df2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuA5PH4WMb2i",
        "outputId": "23ceda3d-b337-4925-d8c9-353df33be5be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+------------+\n",
            "|Seqno|        Name|\n",
            "+-----+------------+\n",
            "|    1|  john jones|\n",
            "|    2|tracey smith|\n",
            "|    3| amy sanders|\n",
            "+-----+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create SparkSession\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com') \\\n",
        "                    .getOrCreate()\n",
        "\n",
        "# Prepare Data\n",
        "columns = [\"Seqno\",\"Name\"]\n",
        "data = [(\"1\", \"john jones\"),\n",
        "    (\"2\", \"tracey smith\"),\n",
        "    (\"3\", \"amy sanders\")]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data=data,schema=columns)\n",
        "df.show()\n",
        "\n",
        "# foreach() Example\n",
        "def f(df):\n",
        "    print(df.Seqno)\n",
        "df.foreach(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ih2C5UpVNEtz"
      },
      "source": [
        "Read Files csv, parquet, Excel, text etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyGLwITHNLzT"
      },
      "outputs": [],
      "source": [
        "# Import\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Read CSV File\n",
        "#df = spark.read.csv(\"sample_data/mnist_test.csv\")\n",
        "#spark.write.csv(\"sample_data/mnist_test1.csv\")\n",
        "#df3 = spark.read.options(header='True', inferSchema='True', delimiter=',') \\\n",
        " # .csv(\"sample_data/mnist_test.csv\")\n",
        "df4 = spark.read.json(\"sample_data/anscombe.json\")\n",
        "df4.printSchema()\n",
        "df4.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddWXqe6i2bQB"
      },
      "source": [
        "*****BroadCast Join*****\n",
        "\n",
        "Broadcast join is an optimization technique in the Spark SQL engine that is used to join two DataFrames. This technique is ideal for joining a large DataFrame with a smaller one. Traditional joins take longer as they require more data shuffling and data is always collected at the driver.\n",
        "\n",
        "* The primary goal of a broadcast join is to eliminate data shuffling and network overhead associated with join operations, which can result in considerable speed benefits.\n",
        "\n",
        "* A broadcast join sends the smaller table (or DataFrame) to all worker nodes, ensuring each worker node has a complete copy of the smaller table in memory.\n",
        "\n",
        "**Types of Broadcast join.**\n",
        "\n",
        "There are two types of broadcast joins.\n",
        "\n",
        "* Broadcast hash joins:     \n",
        "In this case, the driver builds the in-memory hash DataFrame to distribute it to the executors.\n",
        "* Broadcast nested loop join:    \n",
        "It is a nested for-loop join. It is very good for non-equi joins or coalescing joins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbeAPRRd2aUZ",
        "outputId": "7fc079b2-2802-485e-f14a-964337c86bb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+----------+--------+----------+------------+-----+\n",
            "|order_id|product_id|quantity|product_id|product_name|price|\n",
            "+--------+----------+--------+----------+------------+-----+\n",
            "|       1|       101|       2|       101|   Learn C++|   10|\n",
            "|       2|       102|       1|       102|  Mobile: X1|   20|\n",
            "|       3|       103|       3|       103|         LCD|   30|\n",
            "|       4|       101|       1|       101|   Learn C++|   10|\n",
            "|       5|       104|       4|       104|      Laptop|   40|\n",
            "+--------+----------+--------+----------+------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Enable broadcast Join and\n",
        "#Set Threshold limit of size in bytes of a dataFrame to broadcast\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import broadcast\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "sc=spark.sparkContext\n",
        "\n",
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 104857600)\n",
        "\n",
        "#Disable broadcast Join.\n",
        "#spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
        "\n",
        "#Create a Larger DataFrame using weather Dataset in Databricks\n",
        "\n",
        "# Create DataFrames from sample data\n",
        "sales_data = [(1, 101, 2), (2, 102, 1), (3, 103, 3), (4, 101, 1), (5, 104, 4)]\n",
        "products_data = [(101, \"Learn C++\", 10), (102, \"Mobile: X1\", 20), (103, \"LCD\", 30), (104, \"Laptop\", 40)]\n",
        "\n",
        "sales_columns = [\"order_id\", \"product_id\", \"quantity\"]\n",
        "products_columns = [\"product_id\", \"product_name\", \"price\"]\n",
        "\n",
        "sales_df = spark.createDataFrame(sales_data, schema=sales_columns)\n",
        "products_df = spark.createDataFrame(products_data, schema=products_columns)\n",
        "\n",
        "# Perform broadcast join\n",
        "result = sales_df.join(broadcast(products_df), sales_df[\"product_id\"] == products_df[\"product_id\"])\n",
        "\n",
        "# Show result\n",
        "result.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How you can handle the data skewness in Spark**\n",
        "\n",
        "Repartition by Column(s)\n",
        "\n",
        "The first solution is to logically re-partition your data based on the transformations in your script. In short, if you’re grouping or joining, partitioning by the groupBy/join columns can improve shuffle efficiency.\n",
        "\n",
        "df = df.repartition(n_partitions, 'col_1', 'col_2',...)\n",
        "\n",
        "**Spark Optimization Technique**\n",
        "\n",
        "1. Use Serialized data format’s:\n",
        "\n",
        "\n",
        "Most of the Spark jobs run as a pipeline where one Spark job writes data into a File and another Spark jobs read the data, process it, and writes to another file for another Spark job to pick up. When you have such use case, prefer writing an intermediate file in Serialized and optimized formats like Avro, Kryo, Parquet e.t.c, any transformations on these formats performs better than text, CSV, and JSON.\n",
        "\n",
        "2. Persisting & Caching data in memory\n",
        "3. Reduce expensive Shuffle operations like reduceByKey(),groupByKey()\n",
        "\n",
        "\n",
        "**how to handle out of memory exception in spark**\n",
        "\n",
        "Executor Memory Exception Cause:\n",
        "\n",
        "1. YARN Memory Overhead\n",
        "2. High Concurrency\n",
        "3. Large Partitions\n",
        "**Ways to Handle out-of-memory errors**\n",
        "1. Increase cluster resources:\n",
        "\n",
        " If you encounter out-of-memory errors, you can try scaling up your Spark cluster by adding more worker nodes or increasing the resources (CPU, memory) allocated to each worker.\n",
        "\n",
        "2. Partition and cache data:\n",
        "\n",
        " Spark allows you to partition your datasets into smaller chunks. Partitioning helps in distributing the data across the cluster and enables parallel processing.\n",
        "3. Use disk-based storage:\n",
        "\n",
        "  When the dataset is too large to fit entirely in memory, Spark can spill excess data to disk using its disk-based storage mechanism.\n",
        "4. Adjust Spark configurations:\n",
        "\n",
        "Spark provides various configuration parameters that control memory usage. You can tune these parameters to optimize memory allocation based on your specific workload and cluster setup. For example, you can adjust the executor memory (spark.executor.memory), driver memory (spark.driver.memory), and memory overhead (spark.yarn.executor.memoryOverhead) to allocate more memory to Spark processes"
      ],
      "metadata": {
        "id": "whxmKp_YaVdJ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}